{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-30T14:05:02.409622Z",
     "start_time": "2025-04-30T14:05:01.540402Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from keras.src.legacy.backend import dtype\n",
    "from networkx.algorithms.bipartite.basic import color\n",
    "from tensorflow.python.ops.gen_math_ops import squared_difference\n",
    "\n",
    "'''\n",
    "    神经层:\n",
    "    核心神经层(Core layers)\n",
    "    卷积神经层(Convolution layers)\n",
    "    池化神经层(Pooling Layers)\n",
    "    循环神经层(Recurrent Layers)\n",
    "    前置神经层(PreProcessing Layers)\n",
    "    常态化神经层(Normalization Layers)\n",
    "    正则神经层(Regularization Layers)\n",
    "    注意力神经层(Attention Layers)\n",
    "    维度重置神经层(Reshaping Layers)\n",
    "    合并神经层(Merging Layers)\n",
    "    激励神经层(Activation Layers)\n",
    "'''\n",
    "# 完全连接神经层\n",
    "import tensorflow as tf\n",
    "layers = tf.keras.layers\n",
    "\n",
    "#建立模型\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu',name='layer1'),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.Dense(10, activation='softmax',name='layer2')\n",
    "])\n",
    "\n",
    "#设定优化器、损失函数、效果衡量指标\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#显示模型汇总\n",
    "model.summary()"
   ],
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'clip_to_image_size' from 'keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters' (E:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\image_preprocessing\\bounding_boxes\\converters.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 22\u001B[0m\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# 完全连接神经层\u001B[39;00m\n\u001B[0;32m     21\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m layers \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\n\u001B[0;32m     24\u001B[0m \u001B[38;5;66;03m#建立模型\u001B[39;00m\n\u001B[0;32m     25\u001B[0m model \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mmodels\u001B[38;5;241m.\u001B[39mSequential([\n\u001B[0;32m     26\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mFlatten(input_shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m28\u001B[39m, \u001B[38;5;241m28\u001B[39m)),\n\u001B[0;32m     27\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m128\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer1\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m     28\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDropout(\u001B[38;5;241m0.2\u001B[39m),\n\u001B[0;32m     29\u001B[0m     tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlayers\u001B[38;5;241m.\u001B[39mDense(\u001B[38;5;241m10\u001B[39m, activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msoftmax\u001B[39m\u001B[38;5;124m'\u001B[39m,name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlayer2\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     30\u001B[0m ])\n",
      "File \u001B[1;32mE:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:210\u001B[0m, in \u001B[0;36mKerasLazyLoader.__getattr__\u001B[1;34m(self, item)\u001B[0m\n\u001B[0;32m    204\u001B[0m   \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfll_submodule \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfll_submodule\u001B[38;5;241m.\u001B[39mstartswith(\n\u001B[0;32m    205\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__internal__.legacy.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    206\u001B[0m   ):\n\u001B[0;32m    207\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[0;32m    208\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mitem\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` is not available with Keras 3.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    209\u001B[0m     )\n\u001B[1;32m--> 210\u001B[0m module \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_load()\n\u001B[0;32m    211\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(module, item)\n",
      "File \u001B[1;32mE:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\tensorflow\\python\\util\\lazy_loader.py:52\u001B[0m, in \u001B[0;36mLazyLoader._load\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001B[39;00m\n\u001B[0;32m     51\u001B[0m \u001B[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001B[39;00m\n\u001B[1;32m---> 52\u001B[0m module \u001B[38;5;241m=\u001B[39m importlib\u001B[38;5;241m.\u001B[39mimport_module(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfll_parent_module_globals[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_tfll_local_name] \u001B[38;5;241m=\u001B[39m module\n\u001B[0;32m     55\u001B[0m \u001B[38;5;66;03m# Emit a warning if one was specified\u001B[39;00m\n",
      "File \u001B[1;32mE:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\importlib\\__init__.py:90\u001B[0m, in \u001B[0;36mimport_module\u001B[1;34m(name, package)\u001B[0m\n\u001B[0;32m     88\u001B[0m             \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m     89\u001B[0m         level \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m---> 90\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _bootstrap\u001B[38;5;241m.\u001B[39m_gcd_import(name[level:], package, level)\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1310\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1387\u001B[0m, in \u001B[0;36m_gcd_import\u001B[1;34m(name, package, level)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:999\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:488\u001B[0m, in \u001B[0;36m_call_with_frames_removed\u001B[1;34m(f, *args, **kwds)\u001B[0m\n",
      "File \u001B[1;32mE:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\keras\\_tf_keras\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tf_keras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n",
      "File \u001B[1;32mE:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py:30\u001B[0m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m wrappers\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tf_keras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m backend\n\u001B[1;32m---> 30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tf_keras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tf_keras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m losses\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_tf_keras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m metrics\n",
      "File \u001B[1;32mE:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\keras\\_tf_keras\\keras\\layers\\__init__.py:194\u001B[0m\n\u001B[0;32m    188\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_saturation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    189\u001B[0m     RandomSaturation,\n\u001B[0;32m    190\u001B[0m )\n\u001B[0;32m    191\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_sharpness\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    192\u001B[0m     RandomSharpness,\n\u001B[0;32m    193\u001B[0m )\n\u001B[1;32m--> 194\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_shear\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    195\u001B[0m     RandomShear,\n\u001B[0;32m    196\u001B[0m )\n\u001B[0;32m    197\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_translation\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    198\u001B[0m     RandomTranslation,\n\u001B[0;32m    199\u001B[0m )\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom_zoom\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    201\u001B[0m     RandomZoom,\n\u001B[0;32m    202\u001B[0m )\n",
      "File \u001B[1;32mE:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\image_preprocessing\\random_shear.py:5\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi_export\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras_export\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase_image_preprocessing_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     BaseImagePreprocessingLayer,\n\u001B[0;32m      4\u001B[0m )\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbounding_boxes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconverters\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m      6\u001B[0m     clip_to_image_size,\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage_preprocessing\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbounding_boxes\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconverters\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (  \u001B[38;5;66;03m# noqa: E501\u001B[39;00m\n\u001B[0;32m      9\u001B[0m     convert_format,\n\u001B[0;32m     10\u001B[0m )\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msrc\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrandom\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mseed_generator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SeedGenerator\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'clip_to_image_size' from 'keras.src.layers.preprocessing.image_preprocessing.bounding_boxes.converters' (E:\\develop\\anaconda3\\envs\\d2l_tensorflow\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\image_preprocessing\\bounding_boxes\\converters.py)"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-21T13:38:14.778500Z",
     "start_time": "2025-03-21T13:38:14.690591Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "#设定模型的output为第一层Dense,显示第一层Dense output个数(128)\n",
    "#设定模型的input/output\n",
    "feature_extractor = tf.keras.Model(inputs=model.inputs,\n",
    "                                   outputs=model.get_layer('layer2').output)\n",
    "#使用feature_extractor取得output\n",
    "x = tf.ones((1,28,28))\n",
    "features = feature_extractor(x)\n",
    "# features.shape\n",
    "\n",
    "#第一层Dense的参数个数计算\n",
    "parameter_count = 128 * features.shape[1] + features.shape[1]\n",
    "#第一层的参数个数: 100480, 第二层的参数个数: 1290\n",
    "print(f'参数(parameter)个数:{parameter_count}')\n",
    "\n",
    "\n",
    "# Dense层的参数\n",
    "# help(tf.keras.layers.Dense)\n",
    "print(inspect.getsource(tf.keras.layers.Dense))"
   ],
   "id": "bba25ded490f665",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数(parameter)个数:1290\n",
      "@keras_export(\"keras.layers.Dense\")\n",
      "class Dense(Layer):\n",
      "    \"\"\"Just your regular densely-connected NN layer.\n",
      "\n",
      "    `Dense` implements the operation:\n",
      "    `output = activation(dot(input, kernel) + bias)`\n",
      "    where `activation` is the element-wise activation function\n",
      "    passed as the `activation` argument, `kernel` is a weights matrix\n",
      "    created by the layer, and `bias` is a bias vector created by the layer\n",
      "    (only applicable if `use_bias` is `True`).\n",
      "\n",
      "    Note: If the input to the layer has a rank greater than 2, `Dense`\n",
      "    computes the dot product between the `inputs` and the `kernel` along the\n",
      "    last axis of the `inputs` and axis 0 of the `kernel` (using `tf.tensordot`).\n",
      "    For example, if input has dimensions `(batch_size, d0, d1)`, then we create\n",
      "    a `kernel` with shape `(d1, units)`, and the `kernel` operates along axis 2\n",
      "    of the `input`, on every sub-tensor of shape `(1, 1, d1)` (there are\n",
      "    `batch_size * d0` such sub-tensors). The output in this case will have\n",
      "    shape `(batch_size, d0, units)`.\n",
      "\n",
      "    Args:\n",
      "        units: Positive integer, dimensionality of the output space.\n",
      "        activation: Activation function to use.\n",
      "            If you don't specify anything, no activation is applied\n",
      "            (ie. \"linear\" activation: `a(x) = x`).\n",
      "        use_bias: Boolean, whether the layer uses a bias vector.\n",
      "        kernel_initializer: Initializer for the `kernel` weights matrix.\n",
      "        bias_initializer: Initializer for the bias vector.\n",
      "        kernel_regularizer: Regularizer function applied to\n",
      "            the `kernel` weights matrix.\n",
      "        bias_regularizer: Regularizer function applied to the bias vector.\n",
      "        activity_regularizer: Regularizer function applied to\n",
      "            the output of the layer (its \"activation\").\n",
      "        kernel_constraint: Constraint function applied to\n",
      "            the `kernel` weights matrix.\n",
      "        bias_constraint: Constraint function applied to the bias vector.\n",
      "        lora_rank: Optional integer. If set, the layer's forward pass\n",
      "            will implement LoRA (Low-Rank Adaptation)\n",
      "            with the provided rank. LoRA sets the layer's kernel\n",
      "            to non-trainable and replaces it with a delta over the\n",
      "            original kernel, obtained via multiplying two lower-rank\n",
      "            trainable matrices. This can be useful to reduce the\n",
      "            computation cost of fine-tuning large dense layers.\n",
      "            You can also enable LoRA on an existing\n",
      "            `Dense` layer by calling `layer.enable_lora(rank)`.\n",
      "\n",
      "    Input shape:\n",
      "        N-D tensor with shape: `(batch_size, ..., input_dim)`.\n",
      "        The most common situation would be\n",
      "        a 2D input with shape `(batch_size, input_dim)`.\n",
      "\n",
      "    Output shape:\n",
      "        N-D tensor with shape: `(batch_size, ..., units)`.\n",
      "        For instance, for a 2D input with shape `(batch_size, input_dim)`,\n",
      "        the output would have shape `(batch_size, units)`.\n",
      "    \"\"\"\n",
      "\n",
      "    def __init__(\n",
      "        self,\n",
      "        units,\n",
      "        activation=None,\n",
      "        use_bias=True,\n",
      "        kernel_initializer=\"glorot_uniform\",\n",
      "        bias_initializer=\"zeros\",\n",
      "        kernel_regularizer=None,\n",
      "        bias_regularizer=None,\n",
      "        activity_regularizer=None,\n",
      "        kernel_constraint=None,\n",
      "        bias_constraint=None,\n",
      "        lora_rank=None,\n",
      "        **kwargs,\n",
      "    ):\n",
      "        super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "        self.units = units\n",
      "        self.activation = activations.get(activation)\n",
      "        self.use_bias = use_bias\n",
      "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
      "        self.bias_initializer = initializers.get(bias_initializer)\n",
      "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
      "        self.bias_regularizer = regularizers.get(bias_regularizer)\n",
      "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
      "        self.bias_constraint = constraints.get(bias_constraint)\n",
      "        self.lora_rank = lora_rank\n",
      "        self.lora_enabled = False\n",
      "        self.input_spec = InputSpec(min_ndim=2)\n",
      "        self.supports_masking = True\n",
      "\n",
      "    def build(self, input_shape):\n",
      "        input_dim = input_shape[-1]\n",
      "        if self.quantization_mode:\n",
      "            self.quantized_build(input_shape, mode=self.quantization_mode)\n",
      "        if self.quantization_mode != \"int8\":\n",
      "            # If the layer is quantized to int8, `self._kernel` will be added\n",
      "            # in `self._int8_build`. Therefore, we skip it here.\n",
      "            self._kernel = self.add_weight(\n",
      "                name=\"kernel\",\n",
      "                shape=(input_dim, self.units),\n",
      "                initializer=self.kernel_initializer,\n",
      "                regularizer=self.kernel_regularizer,\n",
      "                constraint=self.kernel_constraint,\n",
      "            )\n",
      "        if self.use_bias:\n",
      "            self.bias = self.add_weight(\n",
      "                name=\"bias\",\n",
      "                shape=(self.units,),\n",
      "                initializer=self.bias_initializer,\n",
      "                regularizer=self.bias_regularizer,\n",
      "                constraint=self.bias_constraint,\n",
      "            )\n",
      "        else:\n",
      "            self.bias = None\n",
      "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
      "        self.built = True\n",
      "        if self.lora_rank:\n",
      "            self.enable_lora(self.lora_rank)\n",
      "\n",
      "    @property\n",
      "    def kernel(self):\n",
      "        if not self.built:\n",
      "            raise AttributeError(\n",
      "                \"You must build the layer before accessing `kernel`.\"\n",
      "            )\n",
      "        if self.lora_enabled:\n",
      "            return self._kernel + ops.matmul(\n",
      "                self.lora_kernel_a, self.lora_kernel_b\n",
      "            )\n",
      "        return self._kernel\n",
      "\n",
      "    def call(self, inputs, training=None):\n",
      "        x = ops.matmul(inputs, self.kernel)\n",
      "        if self.bias is not None:\n",
      "            x = ops.add(x, self.bias)\n",
      "        if self.activation is not None:\n",
      "            x = self.activation(x)\n",
      "        return x\n",
      "\n",
      "    def compute_output_shape(self, input_shape):\n",
      "        output_shape = list(input_shape)\n",
      "        output_shape[-1] = self.units\n",
      "        return tuple(output_shape)\n",
      "\n",
      "    def enable_lora(\n",
      "        self, rank, a_initializer=\"he_uniform\", b_initializer=\"zeros\"\n",
      "    ):\n",
      "        if self.kernel_constraint:\n",
      "            raise ValueError(\n",
      "                \"Lora is incompatible with kernel constraints. \"\n",
      "                \"In order to enable lora on this layer, remove the \"\n",
      "                \"`kernel_constraint` argument.\"\n",
      "            )\n",
      "        if not self.built:\n",
      "            raise ValueError(\n",
      "                \"Cannot enable lora on a layer that isn't yet built.\"\n",
      "            )\n",
      "        if self.lora_enabled:\n",
      "            raise ValueError(\n",
      "                \"lora is already enabled. \"\n",
      "                \"This can only be done once per layer.\"\n",
      "            )\n",
      "        self._tracker.unlock()\n",
      "        self.lora_kernel_a = self.add_weight(\n",
      "            name=\"lora_kernel_a\",\n",
      "            shape=(self.kernel.shape[0], rank),\n",
      "            initializer=initializers.get(a_initializer),\n",
      "            regularizer=self.kernel_regularizer,\n",
      "        )\n",
      "        self.lora_kernel_b = self.add_weight(\n",
      "            name=\"lora_kernel_b\",\n",
      "            shape=(rank, self.kernel.shape[1]),\n",
      "            initializer=initializers.get(b_initializer),\n",
      "            regularizer=self.kernel_regularizer,\n",
      "        )\n",
      "        self._kernel.trainable = False\n",
      "        self._tracker.lock()\n",
      "        self.lora_enabled = True\n",
      "        self.lora_rank = rank\n",
      "\n",
      "    def save_own_variables(self, store):\n",
      "        # Do nothing if the layer isn't yet built\n",
      "        if not self.built:\n",
      "            return\n",
      "        # The keys of the `store` will be saved as determined because the\n",
      "        # default ordering will change after quantization\n",
      "        kernel_value, kernel_scale = self._get_kernel_with_merged_lora()\n",
      "        target_variables = [kernel_value]\n",
      "        if self.use_bias:\n",
      "            target_variables.append(self.bias)\n",
      "        if self.quantization_mode is not None:\n",
      "            if self.quantization_mode == \"int8\":\n",
      "                target_variables.append(kernel_scale)\n",
      "            elif self.quantization_mode == \"float8\":\n",
      "                target_variables.append(self.inputs_scale)\n",
      "                target_variables.append(self.inputs_amax_history)\n",
      "                target_variables.append(self.kernel_scale)\n",
      "                target_variables.append(self.kernel_amax_history)\n",
      "                target_variables.append(self.outputs_grad_scale)\n",
      "                target_variables.append(self.outputs_grad_amax_history)\n",
      "            else:\n",
      "                raise self._quantization_mode_error(self.quantization_mode)\n",
      "        for i, variable in enumerate(target_variables):\n",
      "            store[str(i)] = variable\n",
      "\n",
      "    def load_own_variables(self, store):\n",
      "        if not self.lora_enabled:\n",
      "            self._check_load_own_variables(store)\n",
      "        # Do nothing if the layer isn't yet built\n",
      "        if not self.built:\n",
      "            return\n",
      "        # The keys of the `store` will be saved as determined because the\n",
      "        # default ordering will change after quantization\n",
      "        target_variables = [self._kernel]\n",
      "        if self.use_bias:\n",
      "            target_variables.append(self.bias)\n",
      "        if self.quantization_mode is not None:\n",
      "            if self.quantization_mode == \"int8\":\n",
      "                target_variables.append(self.kernel_scale)\n",
      "            elif self.quantization_mode == \"float8\":\n",
      "                target_variables.append(self.inputs_scale)\n",
      "                target_variables.append(self.inputs_amax_history)\n",
      "                target_variables.append(self.kernel_scale)\n",
      "                target_variables.append(self.kernel_amax_history)\n",
      "                target_variables.append(self.outputs_grad_scale)\n",
      "                target_variables.append(self.outputs_grad_amax_history)\n",
      "            else:\n",
      "                raise self._quantization_mode_error(self.quantization_mode)\n",
      "        for i, variable in enumerate(target_variables):\n",
      "            variable.assign(store[str(i)])\n",
      "        if self.lora_enabled:\n",
      "            self.lora_kernel_a.assign(ops.zeros(self.lora_kernel_a.shape))\n",
      "            self.lora_kernel_b.assign(ops.zeros(self.lora_kernel_b.shape))\n",
      "\n",
      "    def get_config(self):\n",
      "        base_config = super().get_config()\n",
      "        config = {\n",
      "            \"units\": self.units,\n",
      "            \"activation\": activations.serialize(self.activation),\n",
      "            \"use_bias\": self.use_bias,\n",
      "            \"kernel_initializer\": initializers.serialize(\n",
      "                self.kernel_initializer\n",
      "            ),\n",
      "            \"bias_initializer\": initializers.serialize(self.bias_initializer),\n",
      "            \"kernel_regularizer\": regularizers.serialize(\n",
      "                self.kernel_regularizer\n",
      "            ),\n",
      "            \"bias_regularizer\": regularizers.serialize(self.bias_regularizer),\n",
      "            \"kernel_constraint\": constraints.serialize(self.kernel_constraint),\n",
      "            \"bias_constraint\": constraints.serialize(self.bias_constraint),\n",
      "        }\n",
      "        if self.lora_rank:\n",
      "            config[\"lora_rank\"] = self.lora_rank\n",
      "        return {**base_config, **config}\n",
      "\n",
      "    def _check_load_own_variables(self, store):\n",
      "        all_vars = self._trainable_variables + self._non_trainable_variables\n",
      "        if len(store.keys()) != len(all_vars):\n",
      "            if len(all_vars) == 0 and not self.built:\n",
      "                raise ValueError(\n",
      "                    f\"Layer '{self.name}' was never built \"\n",
      "                    \"and thus it doesn't have any variables. \"\n",
      "                    f\"However the weights file lists {len(store.keys())} \"\n",
      "                    \"variables for this layer.\\n\"\n",
      "                    \"In most cases, this error indicates that either:\\n\\n\"\n",
      "                    \"1. The layer is owned by a parent layer that \"\n",
      "                    \"implements a `build()` method, but calling the \"\n",
      "                    \"parent's `build()` method did NOT create the state of \"\n",
      "                    f\"the child layer '{self.name}'. A `build()` method \"\n",
      "                    \"must create ALL state for the layer, including \"\n",
      "                    \"the state of any children layers.\\n\\n\"\n",
      "                    \"2. You need to implement \"\n",
      "                    \"the `def build_from_config(self, config)` method \"\n",
      "                    f\"on layer '{self.name}', to specify how to rebuild \"\n",
      "                    \"it during loading. \"\n",
      "                    \"In this case, you might also want to implement the \"\n",
      "                    \"method that generates the build config at saving time, \"\n",
      "                    \"`def get_build_config(self)`. \"\n",
      "                    \"The method `build_from_config()` is meant \"\n",
      "                    \"to create the state \"\n",
      "                    \"of the layer (i.e. its variables) upon deserialization.\",\n",
      "                )\n",
      "            raise ValueError(\n",
      "                f\"Layer '{self.name}' expected {len(all_vars)} variables, \"\n",
      "                \"but received \"\n",
      "                f\"{len(store.keys())} variables during loading. \"\n",
      "                f\"Expected: {[v.name for v in all_vars]}\"\n",
      "            )\n",
      "\n",
      "    # Quantization-related (int8 and float8) methods\n",
      "\n",
      "    def quantized_build(self, input_shape, mode):\n",
      "        if mode == \"int8\":\n",
      "            input_dim = input_shape[-1]\n",
      "            kernel_shape = (input_dim, self.units)\n",
      "            self._int8_build(kernel_shape)\n",
      "        elif mode == \"float8\":\n",
      "            self._float8_build()\n",
      "        else:\n",
      "            raise self._quantization_mode_error(mode)\n",
      "\n",
      "    def _int8_build(\n",
      "        self,\n",
      "        kernel_shape,\n",
      "        kernel_initializer=\"zeros\",\n",
      "        kernel_scale_initializer=\"ones\",\n",
      "    ):\n",
      "        self.inputs_quantizer = quantizers.AbsMaxQuantizer(axis=-1)\n",
      "        self._kernel = self.add_weight(\n",
      "            name=\"kernel\",\n",
      "            shape=kernel_shape,\n",
      "            initializer=kernel_initializer,\n",
      "            dtype=\"int8\",\n",
      "            trainable=False,\n",
      "        )\n",
      "        self.kernel_scale = self.add_weight(\n",
      "            name=\"kernel_scale\",\n",
      "            shape=(self.units,),\n",
      "            initializer=kernel_scale_initializer,\n",
      "            trainable=False,\n",
      "        )\n",
      "        self._is_quantized = True\n",
      "\n",
      "    def _float8_build(self):\n",
      "        from keras.src.dtype_policies import QuantizedFloat8DTypePolicy\n",
      "\n",
      "        # If `self.dtype_policy` is not QuantizedFloat8DTypePolicy, then set\n",
      "        # `amax_history_length` to its default value.\n",
      "        amax_history_length = getattr(\n",
      "            self.dtype_policy,\n",
      "            \"amax_history_length\",\n",
      "            QuantizedFloat8DTypePolicy.default_amax_history_length,\n",
      "        )\n",
      "        # We set `trainable=True` because we will use the gradients to overwrite\n",
      "        # these variables\n",
      "        scale_kwargs = {\n",
      "            \"shape\": (),\n",
      "            \"initializer\": \"ones\",\n",
      "            \"dtype\": \"float32\",  # Always be float32\n",
      "            \"trainable\": True,\n",
      "            \"autocast\": False,\n",
      "        }\n",
      "        amax_history_kwargs = {\n",
      "            \"shape\": (amax_history_length,),\n",
      "            \"initializer\": \"zeros\",\n",
      "            \"dtype\": \"float32\",  # Always be float32\n",
      "            \"trainable\": True,\n",
      "            \"autocast\": False,\n",
      "        }\n",
      "        self.inputs_scale = self.add_weight(name=\"inputs_scale\", **scale_kwargs)\n",
      "        self.inputs_amax_history = self.add_weight(\n",
      "            name=\"inputs_amax_history\", **amax_history_kwargs\n",
      "        )\n",
      "        self.kernel_scale = self.add_weight(name=\"kernel_scale\", **scale_kwargs)\n",
      "        self.kernel_amax_history = self.add_weight(\n",
      "            name=\"kernel_amax_history\", **amax_history_kwargs\n",
      "        )\n",
      "        self.outputs_grad_scale = self.add_weight(\n",
      "            name=\"outputs_grad_scale\", **scale_kwargs\n",
      "        )\n",
      "        self.outputs_grad_amax_history = self.add_weight(\n",
      "            name=\"outputs_grad_amax_history\", **amax_history_kwargs\n",
      "        )\n",
      "        # We need to set `overwrite_with_gradient=True` to instruct the\n",
      "        # optimizer to directly overwrite these variables with their computed\n",
      "        # gradients during training\n",
      "        self.inputs_scale.overwrite_with_gradient = True\n",
      "        self.inputs_amax_history.overwrite_with_gradient = True\n",
      "        self.kernel_scale.overwrite_with_gradient = True\n",
      "        self.kernel_amax_history.overwrite_with_gradient = True\n",
      "        self.outputs_grad_scale.overwrite_with_gradient = True\n",
      "        self.outputs_grad_amax_history.overwrite_with_gradient = True\n",
      "        self._is_quantized = True\n",
      "\n",
      "    def _int8_call(self, inputs, training=None):\n",
      "        @ops.custom_gradient\n",
      "        def matmul_with_inputs_gradient(inputs, kernel, kernel_scale):\n",
      "            def grad_fn(*args, upstream=None):\n",
      "                if upstream is None:\n",
      "                    (upstream,) = args\n",
      "                float_kernel = ops.divide(\n",
      "                    ops.cast(kernel, dtype=self.compute_dtype),\n",
      "                    kernel_scale,\n",
      "                )\n",
      "                inputs_grad = ops.matmul(upstream, ops.transpose(float_kernel))\n",
      "                return (inputs_grad, None, None)\n",
      "\n",
      "            inputs, inputs_scale = self.inputs_quantizer(inputs)\n",
      "            x = ops.matmul(inputs, kernel)\n",
      "            # De-scale outputs\n",
      "            x = ops.cast(x, self.compute_dtype)\n",
      "            x = ops.divide(x, ops.multiply(inputs_scale, kernel_scale))\n",
      "            return x, grad_fn\n",
      "\n",
      "        x = matmul_with_inputs_gradient(\n",
      "            inputs,\n",
      "            ops.convert_to_tensor(self._kernel),\n",
      "            ops.convert_to_tensor(self.kernel_scale),\n",
      "        )\n",
      "        if self.lora_enabled:\n",
      "            lora_x = ops.matmul(inputs, self.lora_kernel_a)\n",
      "            lora_x = ops.matmul(lora_x, self.lora_kernel_b)\n",
      "            x = ops.add(x, lora_x)\n",
      "        if self.bias is not None:\n",
      "            x = ops.add(x, self.bias)\n",
      "        if self.activation is not None:\n",
      "            x = self.activation(x)\n",
      "        return x\n",
      "\n",
      "    def _float8_call(self, inputs, training=None):\n",
      "        if self.lora_enabled:\n",
      "            raise NotImplementedError(\n",
      "                \"Currently, `_float8_call` doesn't support LoRA\"\n",
      "            )\n",
      "\n",
      "        @ops.custom_gradient\n",
      "        def quantized_dequantize_inputs(inputs, scale, amax_history):\n",
      "            if training:\n",
      "                new_scale = quantizers.compute_float8_scale(\n",
      "                    ops.max(amax_history, axis=0),\n",
      "                    scale,\n",
      "                    ops.cast(\n",
      "                        float(ml_dtypes.finfo(\"float8_e4m3fn\").max), \"float32\"\n",
      "                    ),\n",
      "                )\n",
      "                new_amax_history = quantizers.compute_float8_amax_history(\n",
      "                    inputs, amax_history\n",
      "                )\n",
      "            else:\n",
      "                new_scale = None\n",
      "                new_amax_history = None\n",
      "            qdq_inputs = quantizers.quantize_and_dequantize(\n",
      "                inputs, scale, \"float8_e4m3fn\", self.compute_dtype\n",
      "            )\n",
      "\n",
      "            def grad(*args, upstream=None, variables=None):\n",
      "                if upstream is None:\n",
      "                    (upstream,) = args\n",
      "                return upstream, new_scale, new_amax_history\n",
      "\n",
      "            return qdq_inputs, grad\n",
      "\n",
      "        @ops.custom_gradient\n",
      "        def quantized_dequantize_outputs(outputs, scale, amax_history):\n",
      "            \"\"\"Quantize-dequantize the output gradient but not the output.\"\"\"\n",
      "\n",
      "            def grad(*args, upstream=None, variables=None):\n",
      "                if upstream is None:\n",
      "                    (upstream,) = args\n",
      "                new_scale = quantizers.compute_float8_scale(\n",
      "                    ops.max(amax_history, axis=0),\n",
      "                    scale,\n",
      "                    ops.cast(\n",
      "                        float(ml_dtypes.finfo(\"float8_e5m2\").max), \"float32\"\n",
      "                    ),\n",
      "                )\n",
      "                qdq_upstream = quantizers.quantize_and_dequantize(\n",
      "                    upstream, scale, \"float8_e5m2\", self.compute_dtype\n",
      "                )\n",
      "                new_amax_history = quantizers.compute_float8_amax_history(\n",
      "                    upstream, amax_history\n",
      "                )\n",
      "                return qdq_upstream, new_scale, new_amax_history\n",
      "\n",
      "            return outputs, grad\n",
      "\n",
      "        x = ops.matmul(\n",
      "            quantized_dequantize_inputs(\n",
      "                inputs,\n",
      "                ops.convert_to_tensor(self.inputs_scale),\n",
      "                ops.convert_to_tensor(self.inputs_amax_history),\n",
      "            ),\n",
      "            quantized_dequantize_inputs(\n",
      "                ops.convert_to_tensor(self._kernel),\n",
      "                ops.convert_to_tensor(self.kernel_scale),\n",
      "                ops.convert_to_tensor(self.kernel_amax_history),\n",
      "            ),\n",
      "        )\n",
      "        # `quantized_dequantize_outputs` is placed immediately after\n",
      "        # `ops.matmul` for the sake of pattern matching in gemm_rewrite. That\n",
      "        # way, the qdq will be adjacent to the corresponding matmul_bprop in the\n",
      "        # bprop.\n",
      "        x = quantized_dequantize_outputs(\n",
      "            x,\n",
      "            ops.convert_to_tensor(self.outputs_grad_scale),\n",
      "            ops.convert_to_tensor(self.outputs_grad_amax_history),\n",
      "        )\n",
      "        if self.bias is not None:\n",
      "            # Under non-mixed precision cases, F32 bias has to be converted to\n",
      "            # BF16 first to get the biasAdd fusion support. ref. PR\n",
      "            # https://github.com/tensorflow/tensorflow/pull/60306\n",
      "            bias = self.bias\n",
      "            if self.dtype_policy.compute_dtype == \"float32\":\n",
      "                bias_bf16 = ops.cast(bias, \"bfloat16\")\n",
      "                bias = ops.cast(bias_bf16, bias.dtype)\n",
      "            x = ops.add(x, bias)\n",
      "        if self.activation is not None:\n",
      "            x = self.activation(x)\n",
      "        return x\n",
      "\n",
      "    def quantize(self, mode, type_check=True):\n",
      "        # Prevent quantization of the subclasses\n",
      "        if type_check and (type(self) is not Dense):\n",
      "            raise self._not_implemented_error(self.quantize)\n",
      "\n",
      "        if mode == \"int8\":\n",
      "            # Quantize `self._kernel` to int8 and compute corresponding scale\n",
      "            kernel_value, kernel_scale = quantizers.abs_max_quantize(\n",
      "                self._kernel, axis=0, to_numpy=True\n",
      "            )\n",
      "            kernel_scale = ops.squeeze(kernel_scale, axis=0)\n",
      "            kernel_shape = tuple(self._kernel.shape)\n",
      "            del self._kernel\n",
      "            # Utilize a lambda expression as an initializer to prevent adding a\n",
      "            # large constant to the computation graph.\n",
      "            self._int8_build(kernel_shape, kernel_value, kernel_scale)\n",
      "        elif mode == \"float8\":\n",
      "            self._float8_build()\n",
      "        else:\n",
      "            raise self._quantization_mode_error(mode)\n",
      "\n",
      "        # Set new dtype policy\n",
      "        if self.dtype_policy.quantization_mode is None:\n",
      "            policy = dtype_policies.get(f\"{mode}_from_{self.dtype_policy.name}\")\n",
      "            self.dtype_policy = policy\n",
      "\n",
      "    def _get_kernel_with_merged_lora(self):\n",
      "        if self.dtype_policy.quantization_mode is not None:\n",
      "            kernel_value = self._kernel\n",
      "            kernel_scale = self.kernel_scale\n",
      "            if self.lora_enabled:\n",
      "                # Dequantize & quantize to merge lora weights into int8 kernel\n",
      "                # Note that this is a lossy compression\n",
      "                kernel_value = ops.divide(kernel_value, kernel_scale)\n",
      "                kernel_value = ops.add(\n",
      "                    kernel_value,\n",
      "                    ops.matmul(self.lora_kernel_a, self.lora_kernel_b),\n",
      "                )\n",
      "                kernel_value, kernel_scale = quantizers.abs_max_quantize(\n",
      "                    kernel_value, axis=0, to_numpy=True\n",
      "                )\n",
      "                kernel_scale = ops.squeeze(kernel_scale, axis=0)\n",
      "            return kernel_value, kernel_scale\n",
      "        return self.kernel, None\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:13:36.362Z",
     "start_time": "2025-03-23T14:13:36.035724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.api import activations\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "'''\n",
    "    激活函数\n",
    "'''\n",
    "x = np.linspace(-10,10,21)\n",
    "x_tf = tf.constant(x,dtype=tf.float32)\n",
    "\n",
    "#relu\n",
    "'''\n",
    "    relu函数有三个参数\n",
    "    threshold: 超过此阈值,y才会大于0.\n",
    "    max_value: y的上限\n",
    "    alpha: 小于阈值,y = α * x。 α = 0.5,称为Parameteric rectified linear unit(PReLU),\n",
    "    alpha = 0.01,则称为Leaky rectified linear unit(Leaky ReLU)\n",
    "'''\n",
    "y = activations.relu(x_tf).numpy()\n",
    "\n",
    "\n",
    "#绘图\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ],
   "id": "eb9416467777122d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:13:48.340879Z",
     "start_time": "2025-03-23T14:13:48.012705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "from keras.api import activations\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "'''\n",
    "sigmoid: Logistic回归,函数为S型,适用于二分类,可加在最后一层Dense中\n",
    "    函数最小值是0,最大值是1,两条直线之间是模糊地带,但也是一个平滑改变的过程,\n",
    "    而非阶梯型的函数,可降低预测的变异性\n",
    "'''\n",
    "x = np.linspace(-10,10,21)\n",
    "x_tf = tf.constant(x,dtype=tf.float32)\n",
    "\n",
    "y = activations.sigmoid(x_tf).numpy()\n",
    "\n",
    "#模糊地带\n",
    "plt.axvline(-4,color='r')\n",
    "plt.axvline(4,color='r')\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "39942ef720108a6b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T14:15:52.930632Z",
     "start_time": "2025-03-23T14:15:52.558243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.api import activations\n",
    "'''\n",
    "    tanh函数,平滑改变的过程与sigmoid相比较为陡峭\n",
    "'''\n",
    "x = np.linspace(-10,10,21)\n",
    "x_tf = tf.constant(x,dtype=tf.float32)\n",
    "\n",
    "y = activations.tanh(x_tf).numpy()\n",
    "\n",
    "#模糊地带\n",
    "plt.axvline(-3,color='r')\n",
    "plt.axvline(3,color='r')\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ],
   "id": "c5c92d3da0b105e1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T13:03:58.552191Z",
     "start_time": "2025-03-24T13:03:39.366171Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.api import activations\n",
    "'''\n",
    "    softmax函数:这个函数会将输入转化为概率,即所有值介于(0,1),总和为1,适用于多分类,可\n",
    "    加在最后一层Dense中。\n",
    "'''\n",
    "#设定x为均匀分布,转换后,每一行的总和为1.`\n",
    "x = np.random.uniform(1,10,40).reshape(10,4)\n",
    "print('输入: \\n',x)\n",
    "x_tf = tf.constant(x,dtype=tf.float32)\n",
    "\n",
    "# softmax,输入必须是二维数据\n",
    "y = activations.softmax(x_tf).numpy()\n",
    "print('加总:', np.round(np.sum(y,axis=1)))"
   ],
   "id": "4304b6059e07936d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入: \n",
      " [[1.21024542 9.3281665  3.92997095 4.87891147]\n",
      " [6.98387316 1.31836089 4.20305793 6.10498752]\n",
      " [5.42465634 8.55866683 4.2664356  1.71796206]\n",
      " [1.86053736 2.34403409 1.61989553 8.68046836]\n",
      " [5.0757642  8.0512366  4.812716   9.69284888]\n",
      " [2.14840751 9.21109295 5.50008592 1.44249783]\n",
      " [6.64726779 3.51683485 6.05260404 8.70035197]\n",
      " [9.35808072 8.56828669 8.06806399 4.00509118]\n",
      " [6.51609823 3.77739661 2.73316854 6.33756821]\n",
      " [9.35879876 2.9887897  4.05497118 4.27881553]]\n",
      "加总: [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-25T14:15:01.166732Z",
     "start_time": "2025-03-25T14:14:56.184629Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8149245\n",
      "===============================多分类的交叉熵====================\n",
      "1.1769392\n",
      "===============================稀疏矩阵的多分类交叉熵====================\n",
      "1.1769392\n",
      "==============MeanSquaredError======================\n",
      "0.25\n",
      "===================铰链损失函数======================\n",
      "1.3\n",
      "========================自定义损失函数=================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[5], line 56\u001B[0m\n\u001B[0;32m     53\u001B[0m     squared_difference \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39msquare(y_true \u001B[38;5;241m-\u001B[39m y_pred)\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mreduce_mean(squared_difference, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m---> 56\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(optimizer\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124madam\u001B[39m\u001B[38;5;124m'\u001B[39m, loss\u001B[38;5;241m=\u001B[39mmy_loss)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'model' is not defined"
     ]
    }
   ],
   "execution_count": 5,
   "source": [
    "'''\n",
    "    损失函数(Loss Functions),tensorflow损失函数分为三类:\n",
    "    1)概率相关的损失函数(Probabilistic Loss): 如二分类的交叉熵(Binary Crossentropy)、多分类的交叉熵(CategoticalCrossentropy)\n",
    "    2)回归相关的损失函数(Regression Loss): 如均方误差\n",
    "    3)铰链损失函数(Hinge Loss)。经常用在最大间格分类,适用于支持向量机(SVM)等算法\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "# 使用sigmoid计算，就可以得到BinaryCrossentropy损失函数\n",
    "y_true = [[0.,1.],[0.,0.]] #实际值\n",
    "y_pred = [[0.6,0.4],[0.4,0.6]]\n",
    "\n",
    "# 二分类的交叉熵损失函数\n",
    "bce = tf.keras.losses.BinaryCrossentropy()\n",
    "print(bce(y_true,y_pred).numpy())\n",
    "\n",
    "print(\"===============================多分类的交叉熵====================\")\n",
    "y_true = [[0.,1.,0.],[0.,0.,1.]]\n",
    "y_pred = [[0.05,0.95,0.],[0.8,0.1,0.1]]\n",
    "\n",
    "#多分类交叉熵(CategoricalCrossentropy)\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "print(cce(y_true,y_pred).numpy())\n",
    "\n",
    "print(\"===============================稀疏矩阵的多分类交叉熵====================\")\n",
    "y_true = tf.constant([1,2],dtype=tf.float32)  # 稀疏标签应该是0-based索引\n",
    "y_pred = tf.constant([[0.05,0.95,0],[0.8,0.1,0.1]],dtype=tf.float32)\n",
    "\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "print(scce(y_true,y_pred).numpy())\n",
    "\n",
    "print(\"==============MeanSquaredError======================\")\n",
    "#MeanSquaredError,计算实际值与预测值的均方误差\n",
    "y_true = [[0.,1.],[0.,0.]] #实际值\n",
    "y_pred = [[1.,1.],[0.,0.]] #预测值\n",
    "\n",
    "#多分类交叉熵(CategoricalCrossentropy)\n",
    "mse = tf.keras.losses.MeanSquaredError()\n",
    "print(mse(y_true,y_pred).numpy())\n",
    "\n",
    "print(\"===================铰链损失函数======================\")\n",
    "#常用于支持向量机\n",
    "y_true = [[0.,1],[0.,0.]]\n",
    "y_pred = [[0.6,0.4],[0.4,0.6]]\n",
    "\n",
    "hinge = tf.keras.losses.Hinge()\n",
    "print(hinge(y_true,y_pred).numpy())\n",
    "\n",
    "print(\"========================自定义损失函数=================\")\n",
    "def my_loss(y_true,y_pred):\n",
    "    #MSE\n",
    "    squared_difference = tf.square(y_true - y_pred)\n",
    "    return tf.reduce_mean(squared_difference, axis=-1)\n",
    "\n",
    "# model.compile(optimizer='adam', loss=my_loss)"
   ],
   "id": "62d2f121b23da6bb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-26T14:04:22.237942Z",
     "start_time": "2025-03-26T14:04:03.202979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    优化器: 是神经网络中反向传导的求解方法,主要用在以下两方面,\n",
    "    1)设定学习率的变化,加速求解的收敛速度。\n",
    "    2)避开马鞍点(Saddle Point)等局部最小值,并且找到全局的最小值\n",
    "    优化器的类别在model.compile()中设置\n",
    "    常见的优化器:\n",
    "    SGD\n",
    "    RMSprop\n",
    "    Adam\n",
    "    AdadeLta\n",
    "    Adagrad\n",
    "    Adamax\n",
    "    Nadam\n",
    "    Ftrl\n",
    "'''\n",
    "'''\n",
    "    1.随机梯度下降法\n",
    "    依据权重更新的时机差别,梯度下降法分为三种:\n",
    "    1)批量梯度下降法(BGD)\n",
    "    2)随机梯度下降法\n",
    "    3)小批量梯度下降法\n",
    "    小批量梯度下降法可能包含前两种,批量为全部样本,即为BGD;批量为1,即为SGD。\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "#SGD\n",
    "tf.keras.optimizers.SGD(\n",
    "    learning_rate=0.01,\n",
    "    momentum=0.0,\n",
    "    nesterov=False,\n",
    "    name=\"SGD\",\n",
    ")\n",
    "# ① 权重更新公式为: w = w -learning_rate * g  w为权重,g为梯度,learning_rate为学习率\n",
    "# ② momentum为动量,momentum=0,则不考虑动量, momentum=1,则考虑动量, momentum=0.9,则考虑动量\n",
    "#   velocity = momentum * velocity - learning_rate * g\n",
    "#   w = w + velocity  动能通常介于(0,1),若momentum=0,则学习率为固定值,一般刚开始学习率比较大,越接近最小值时,学习率\n",
    "#   越小,以免错过最小值\n",
    "\n",
    "\n",
    "help(tf.keras.optimizers.SGD)"
   ],
   "id": "6556d7161f141547",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SGD in module keras.src.optimizers.sgd:\n",
      "\n",
      "class SGD(keras.src.optimizers.optimizer.Optimizer)\n",
      " |  SGD(learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, loss_scale_factor=None, gradient_accumulation_steps=None, name='SGD', **kwargs)\n",
      " |\n",
      " |  Gradient descent (with momentum) optimizer.\n",
      " |\n",
      " |  Update rule for parameter `w` with gradient `g` when `momentum` is 0:\n",
      " |\n",
      " |  ```python\n",
      " |  w = w - learning_rate * g\n",
      " |  ```\n",
      " |\n",
      " |  Update rule when `momentum` is larger than 0:\n",
      " |\n",
      " |  ```python\n",
      " |  velocity = momentum * velocity - learning_rate * g\n",
      " |  w = w + velocity\n",
      " |  ```\n",
      " |\n",
      " |  When `nesterov=True`, this rule becomes:\n",
      " |\n",
      " |  ```python\n",
      " |  velocity = momentum * velocity - learning_rate * g\n",
      " |  w = w + momentum * velocity - learning_rate * g\n",
      " |  ```\n",
      " |\n",
      " |  Args:\n",
      " |      learning_rate: A float, a\n",
      " |          `keras.optimizers.schedules.LearningRateSchedule` instance, or\n",
      " |          a callable that takes no arguments and returns the actual value to\n",
      " |          use. The learning rate. Defaults to `0.01`.\n",
      " |      momentum: float hyperparameter >= 0 that accelerates gradient descent in\n",
      " |          the relevant direction and dampens oscillations. 0 is vanilla\n",
      " |          gradient descent. Defaults to `0.0`.\n",
      " |      nesterov: boolean. Whether to apply Nesterov momentum.\n",
      " |          Defaults to `False`.\n",
      " |      name: String. The name to use\n",
      " |          for momentum accumulator weights created by\n",
      " |          the optimizer.\n",
      " |      weight_decay: Float. If set, weight decay is applied.\n",
      " |      clipnorm: Float. If set, the gradient of each weight is individually\n",
      " |          clipped so that its norm is no higher than this value.\n",
      " |      clipvalue: Float. If set, the gradient of each weight is clipped to be\n",
      " |          no higher than this value.\n",
      " |      global_clipnorm: Float. If set, the gradient of all weights is clipped\n",
      " |          so that their global norm is no higher than this value.\n",
      " |      use_ema: Boolean, defaults to `False`.\n",
      " |          If `True`, exponential moving average\n",
      " |          (EMA) is applied. EMA consists of computing an exponential moving\n",
      " |          average of the weights of the model (as the weight values change\n",
      " |          after each training batch), and periodically overwriting the\n",
      " |          weights with their moving average.\n",
      " |      ema_momentum: Float, defaults to 0.99. Only used if `use_ema=True`.\n",
      " |          This is the momentum to use when computing\n",
      " |          the EMA of the model's weights:\n",
      " |          `new_average = ema_momentum * old_average + (1 - ema_momentum) *\n",
      " |          current_variable_value`.\n",
      " |      ema_overwrite_frequency: Int or None, defaults to None. Only used if\n",
      " |          `use_ema=True`. Every `ema_overwrite_frequency` steps of iterations,\n",
      " |          we overwrite the model variable by its moving average.\n",
      " |          If None, the optimizer\n",
      " |          does not overwrite model variables in the middle of training,\n",
      " |          and you need to explicitly overwrite the variables\n",
      " |          at the end of training by calling\n",
      " |          `optimizer.finalize_variable_values()` (which updates the model\n",
      " |          variables in-place). When using the built-in `fit()` training loop,\n",
      " |          this happens automatically after the last epoch,\n",
      " |          and you don't need to do anything.\n",
      " |      loss_scale_factor: Float or `None`. If a float, the scale factor will\n",
      " |          be multiplied the loss before computing gradients, and the inverse\n",
      " |          of the scale factor will be multiplied by the gradients before\n",
      " |          updating variables. Useful for preventing underflow during\n",
      " |          mixed precision training. Alternately,\n",
      " |          `keras.optimizers.LossScaleOptimizer` will\n",
      " |          automatically set a loss scale factor.\n",
      " |      gradient_accumulation_steps: Int or `None`. If an int, model & optimizer\n",
      " |          variables will not be updated at every step; instead they will be\n",
      " |          updated every `gradient_accumulation_steps` steps, using the average\n",
      " |          value of the gradients since the last update. This is known as\n",
      " |          \"gradient accumulation\". This can be useful\n",
      " |          when your batch size is very small, in order to reduce gradient\n",
      " |          noise at each update step. EMA frequency will look at \"accumulated\"\n",
      " |          iterations value (optimizer steps // gradient_accumulation_steps).\n",
      " |          Learning rate schedules will look at \"real\" iterations value\n",
      " |          (optimizer steps).\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      SGD\n",
      " |      keras.src.optimizers.optimizer.Optimizer\n",
      " |      keras.src.backend.tensorflow.optimizer.TFOptimizer\n",
      " |      keras.src.backend.tensorflow.trackable.KerasAutoTrackable\n",
      " |      tensorflow.python.trackable.autotrackable.AutoTrackable\n",
      " |      tensorflow.python.trackable.base.Trackable\n",
      " |      keras.src.optimizers.base_optimizer.BaseOptimizer\n",
      " |      keras.src.saving.keras_saveable.KerasSaveable\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, learning_rate=0.01, momentum=0.0, nesterov=False, weight_decay=None, clipnorm=None, clipvalue=None, global_clipnorm=None, use_ema=False, ema_momentum=0.99, ema_overwrite_frequency=None, loss_scale_factor=None, gradient_accumulation_steps=None, name='SGD', **kwargs)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |\n",
      " |  build(self, variables)\n",
      " |      Initialize optimizer variables.\n",
      " |\n",
      " |      SGD optimizer has one variable `momentums`, only set if `self.momentum`\n",
      " |      is not 0.\n",
      " |\n",
      " |      Args:\n",
      " |        var_list: list of model variables to build SGD variables on.\n",
      " |\n",
      " |  get_config(self)\n",
      " |      Returns the config of the optimizer.\n",
      " |\n",
      " |      An optimizer config is a Python dictionary (serializable)\n",
      " |      containing the configuration of an optimizer.\n",
      " |      The same optimizer can be reinstantiated later\n",
      " |      (without any saved state) from this configuration.\n",
      " |\n",
      " |      Subclass optimizer should override this method to include other\n",
      " |      hyperparameters.\n",
      " |\n",
      " |      Returns:\n",
      " |          Python dictionary.\n",
      " |\n",
      " |  update_step(self, gradient, variable, learning_rate)\n",
      " |      Update step given gradient and the associated model variable.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.backend.tensorflow.optimizer.TFOptimizer:\n",
      " |\n",
      " |  add_variable_from_reference(self, reference_variable, name=None, initializer='zeros')\n",
      " |      Add an optimizer variable from the model variable.\n",
      " |\n",
      " |      Create an optimizer variable based on the information of model variable.\n",
      " |      For example, in SGD optimizer momemtum, for each model variable, a\n",
      " |      corresponding momemtum variable is created of the same shape and dtype.\n",
      " |\n",
      " |      Args:\n",
      " |          reference_variable: `keras.Variable`. The corresponding model\n",
      " |              variable to the optimizer variable to be created.\n",
      " |          name: Optional string. The name prefix of the optimizer variable to\n",
      " |              be created. If not provided, it will be set to `\"var\"`. The\n",
      " |              variable name will follow the pattern\n",
      " |              `{variable_name}_{reference_variable.name}`,\n",
      " |              e.g., `momemtum/dense_1`. Defaults to `None`.\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). If unspecified, defaults to\n",
      " |              `\"zeros\"`.\n",
      " |\n",
      " |      Returns:\n",
      " |          An optimizer variable, in the format of `keras.Variable`.\n",
      " |\n",
      " |  assign(self, variable, value)\n",
      " |      Assign a value to a variable.\n",
      " |\n",
      " |      This should be used in optimizers instead of `variable.assign(value)` to\n",
      " |      support backend specific optimizations.\n",
      " |      Note that the variable can be a model variable or an optimizer variable;\n",
      " |      it can be a backend native variable or a Keras variable.\n",
      " |\n",
      " |      Args:\n",
      " |          variable: The variable to update.\n",
      " |          value: The value to add to the variable.\n",
      " |\n",
      " |  assign_add(self, variable, value)\n",
      " |      Add a value to a variable.\n",
      " |\n",
      " |      This should be used in optimizers instead of\n",
      " |      `variable.assign_add(value)` to support backend specific optimizations.\n",
      " |      Note that the variable can be a model variable or an optimizer variable;\n",
      " |      it can be a backend native variable or a Keras variable.\n",
      " |\n",
      " |      Args:\n",
      " |          variable: The variable to update.\n",
      " |          value: The value to add to the variable.\n",
      " |\n",
      " |  assign_sub(self, variable, value)\n",
      " |      Subtract a value from a variable.\n",
      " |\n",
      " |      This should be used in optimizers instead of\n",
      " |      `variable.assign_sub(value)` to support backend specific optimizations.\n",
      " |      Note that the variable can be a model variable or an optimizer variable;\n",
      " |      it can be a backend native variable or a Keras variable.\n",
      " |\n",
      " |      Args:\n",
      " |          variable: The variable to update.\n",
      " |          value: The value to add to the variable.\n",
      " |\n",
      " |  stateless_apply(self, optimizer_variables, grads, trainable_variables)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.backend.tensorflow.trackable.KerasAutoTrackable:\n",
      " |\n",
      " |  __setattr__(self, name, value)\n",
      " |      Support self.foo = trackable syntax.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from tensorflow.python.trackable.autotrackable.AutoTrackable:\n",
      " |\n",
      " |  __delattr__(self, name)\n",
      " |      Implement delattr(self, name).\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from tensorflow.python.trackable.base.Trackable:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.optimizers.base_optimizer.BaseOptimizer:\n",
      " |\n",
      " |  add_variable(self, shape, initializer='zeros', dtype=None, aggregation='none', name=None)\n",
      " |      Add a variable to the optimizer.\n",
      " |\n",
      " |      Args:\n",
      " |          shape: Shape tuple for the variable. Must be fully-defined\n",
      " |              (no `None` entries).\n",
      " |          initializer: Initializer object to use to populate the initial\n",
      " |              variable value, or string name of a built-in initializer\n",
      " |              (e.g. `\"random_normal\"`). Defaults to `\"zeros\"`.\n",
      " |          dtype: Dtype of the variable to create, e.g. `\"float32\"`. If\n",
      " |              unspecified, defaults to the `keras.backend.floatx()`.\n",
      " |          aggregation: Optional string, one of `None`, `\"none\"`, `\"mean\"`,\n",
      " |              `\"sum\"` or `\"only_first_replica\"`. Annotates the variable with\n",
      " |              the type of multi-replica aggregation to be used for this\n",
      " |              variable when writing custom data parallel training loops.\n",
      " |              Defaults to `\"none\"`.\n",
      " |          name: String name of the variable. Useful for debugging purposes.\n",
      " |\n",
      " |      Returns:\n",
      " |          An optimizer variable, in the format of `keras.Variable`.\n",
      " |\n",
      " |  apply(self, grads, trainable_variables=None)\n",
      " |      Update traininable variables according to provided gradient values.\n",
      " |\n",
      " |      `grads` should be a list of gradient tensors\n",
      " |      with 1:1 mapping to the list of variables the optimizer was built with.\n",
      " |\n",
      " |      `trainable_variables` can be provided\n",
      " |      on the first call to build the optimizer.\n",
      " |\n",
      " |  apply_gradients(self, grads_and_vars)\n",
      " |\n",
      " |  exclude_from_weight_decay(self, var_list=None, var_names=None)\n",
      " |      Exclude variables from weight decay.\n",
      " |\n",
      " |      This method must be called before the optimizer's `build` method is\n",
      " |      called. You can set specific variables to exclude out, or set a list of\n",
      " |      strings as the anchor words, if any of which appear in a variable's\n",
      " |      name, then the variable is excluded.\n",
      " |\n",
      " |      Args:\n",
      " |          var_list: A list of `Variable`s to exclude from weight decay.\n",
      " |          var_names: A list of strings. If any string in `var_names` appear\n",
      " |              in the model variable's name, then this model variable is\n",
      " |              excluded from weight decay. For example, `var_names=['bias']`\n",
      " |              excludes all bias variables from weight decay.\n",
      " |\n",
      " |  finalize_variable_values(self, var_list)\n",
      " |      Set the final value of model's trainable variables.\n",
      " |\n",
      " |      Sometimes there are some extra steps before ending the variable updates,\n",
      " |      such as overriding the model variables with its average value.\n",
      " |\n",
      " |      Args:\n",
      " |        var_list: list of model variables.\n",
      " |\n",
      " |  load_own_variables(self, store)\n",
      " |      Set the state of this optimizer object.\n",
      " |\n",
      " |  save_own_variables(self, store)\n",
      " |      Get the state of this optimizer object.\n",
      " |\n",
      " |  scale_loss(self, loss)\n",
      " |      Scale the loss before computing gradients.\n",
      " |\n",
      " |      Scales the loss before gradients are computed in a `train_step`. This\n",
      " |      is primarily useful during mixed precision training to prevent numeric\n",
      " |      underflow.\n",
      " |\n",
      " |  set_weights(self, weights)\n",
      " |      Set the weights of the optimizer.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from keras.src.optimizers.base_optimizer.BaseOptimizer:\n",
      " |\n",
      " |  from_config(config, custom_objects=None)\n",
      " |      Creates an optimizer from its config.\n",
      " |\n",
      " |      This method is the reverse of `get_config`, capable of instantiating the\n",
      " |      same optimizer from the config dictionary.\n",
      " |\n",
      " |      Args:\n",
      " |          config: A Python dictionary, typically the output of get_config.\n",
      " |          custom_objects: A Python dictionary mapping names to additional\n",
      " |            user-defined Python objects needed to recreate this optimizer.\n",
      " |\n",
      " |      Returns:\n",
      " |          An optimizer instance.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from keras.src.optimizers.base_optimizer.BaseOptimizer:\n",
      " |\n",
      " |  iterations\n",
      " |\n",
      " |  variables\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras.src.optimizers.base_optimizer.BaseOptimizer:\n",
      " |\n",
      " |  learning_rate\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras.src.saving.keras_saveable.KerasSaveable:\n",
      " |\n",
      " |  __reduce__(self)\n",
      " |      __reduce__ is used to customize the behavior of `pickle.pickle()`.\n",
      " |\n",
      " |      The method returns a tuple of two elements: a function, and a list of\n",
      " |      arguments to pass to that function.  In this case we just leverage the\n",
      " |      keras saving library.\n",
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T13:52:19.320848Z",
     "start_time": "2025-03-27T13:52:18.812547Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# 定义优化器、变量、损失函数\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "var = tf.Variable(1.0)\n",
    "loss = lambda: (var ** 2) / 2.0  # 损失函数简化为 var（因为 (var*2)/2 = var）\n",
    "\n",
    "for i in range(51):\n",
    "    # Tensorflow2.X的SGD优化器中没有minimize方法,使用下面的代码\n",
    "    # step_count = opt.minimize(loss, [var]).numpy()\n",
    "    with tf.GradientTape() as tape:\n",
    "        #计算当前的损失值,用于比较与模型预测值与真实值之间的差距\n",
    "        current_loss = loss()\n",
    "    #基于损失值,找调整方向,tape利用前面记录的操作,算出损失current_loss对var的敏感度,\n",
    "    # 也就是梯度grads。类似于梯度告诉你,如果我爸参数var稍微改一点,损失会变大还是变小,变多少\n",
    "    # [var]是一个参数列表.grads也是一个列表,里面存放着每个参数的调整建议。\n",
    "    grads = tape.gradient(current_loss, [var])\n",
    "    # apply_gradients根据grads的调整建议进行调参,让误差变小,opt是优化器\n",
    "    opt.apply_gradients(zip(grads, [var]))\n",
    "    if i % 10 == 0 and i > 0:\n",
    "        print(f\"Step {i}: {var.numpy()}\")\n",
    "\n",
    "#随着执行步数增多,损失值越来越小"
   ],
   "id": "6c00a4e5ba60465b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10: 0.3138105869293213\n",
      "Step 20: 0.10941898822784424\n",
      "Step 30: 0.03815204277634621\n",
      "Step 40: 0.013302796520292759\n",
      "Step 50: 0.004638398066163063\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:16:22.512598Z",
     "start_time": "2025-03-27T14:16:22.449777Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    优化三次测试随机梯度下降法的动能\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "var = tf.Variable(1.0)\n",
    "\n",
    "#损失函数起始值\n",
    "val0 = var.value()\n",
    "print(f'val0:{val0}')\n",
    "\n",
    "#损失函数\n",
    "loss = lambda : (var ** 2) / 2.0\n",
    "\n",
    "for i in range(3):\n",
    "    initValue = var\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss()\n",
    "    grads = tape.gradient(current_loss, [var])\n",
    "    opt.apply_gradients(zip(grads, [var]))\n",
    "    if i > 0:\n",
    "         print(f\"Step {i}: {var.numpy()},---->{(initValue - var).numpy()}\")"
   ],
   "id": "4fd46e90e19233f9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val0:1.0\n",
      "Step 1: 0.7199999690055847,---->0.0\n",
      "Step 2: 0.4860000014305115,---->0.0\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-27T14:26:36.244688Z",
     "start_time": "2025-03-27T14:26:36.001446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    Adam优化器\n",
    "'''\n",
    "import tensorflow as tf\n",
    "tf.keras.optimizers.Adam(\n",
    "    learning_rate=0.001, #学习率\n",
    "    beta_1=0.9, #一阶动能衰减率\n",
    "    beta_2=0.999, # 二阶动能衰减率\n",
    "    epsilon=1e-7, # 误差值,小于这个值,优化即停止\n",
    "    amsgrad=False, # 是否使用AMSGrad,默认值是False\n",
    "    name=\"Adam\", #\n",
    ")\n",
    "\n",
    "#Adam\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.1,epsilon=0.1)\n",
    "# 定义变量和损失函数\n",
    "var = tf.Variable(1.0)\n",
    "loss = lambda: (var ** 2) / 2.0\n",
    "\n",
    "#优化的步骤\n",
    "for i in range(12):\n",
    "    with tf.GradientTape() as tape:\n",
    "        current_loss = loss()\n",
    "    grads = tape.gradient(current_loss,[var])\n",
    "    opt.apply_gradients(zip(grads,[var]))\n",
    "    if i >= 0:\n",
    "        print(f\"Step {i}: {var.numpy()}\")"
   ],
   "id": "f5f52b782d6663fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: 0.9759748578071594\n",
      "Step 1: 0.9453564882278442\n",
      "Step 2: 0.9106557369232178\n",
      "Step 3: 0.8731074929237366\n",
      "Step 4: 0.8334801197052002\n",
      "Step 5: 0.7923160791397095\n",
      "Step 6: 0.7500312924385071\n",
      "Step 7: 0.706963300704956\n",
      "Step 8: 0.6633984446525574\n",
      "Step 9: 0.6195876598358154\n",
      "Step 10: 0.5757566690444946\n",
      "Step 11: 0.5321123600006104\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-07T14:10:58.459280Z",
     "start_time": "2025-04-07T14:10:58.389443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "优化三次测试随机梯度下降法的功能\n",
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "var = tf.Variable(1.0)\n",
    "#损失函数的起始值\n",
    "val0 = var.value()\n",
    "print(f'val0:{val0}')\n",
    "\n",
    "#损失函数\n",
    "loss = lambda : (var ** 2) / 2.0\n",
    "#优化第一次,minimize函数是tf1.X的功能\n",
    "# step_count = opt.minimize(loss, [var]).numpy()\n",
    "with tf.GradientTape() as tape:\n",
    "    #计算损失\n",
    "    current_loss = loss()\n",
    "#计算梯度\n",
    "grads = tape.gradient(current_loss, [var])\n",
    "#应用梯度更新变量\n",
    "opt.apply_gradients(zip(grads, [var]))\n",
    "val1 = var.numpy()\n",
    "print(f'优化的步骤:{1},val1:{val1},变化值:{val0 - val1}')\n",
    "\n",
    "#优化第二次\n",
    "with tf.GradientTape() as tape:\n",
    "    current_loss = loss()\n",
    "grads = tape.gradient(current_loss, [var])\n",
    "opt.apply_gradients(zip(grads, [var]))\n",
    "val2 = var.numpy()\n",
    "print(f'优化的步骤:{2},val2:{val2},变化值:{val1 - val2}')\n",
    "\n",
    "#优化第三次\n",
    "with tf.GradientTape() as tape:\n",
    "    current_loss = loss()\n",
    "grads = tape.gradient(current_loss, [var])\n",
    "opt.apply_gradients(zip(grads, [var]))\n",
    "val3 = var.numpy()\n",
    "print(f'优化的步骤:{3},val3:{val3},变化值:{val2 - val3}')"
   ],
   "id": "8d819fbd1e01798",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val0:1.0\n",
      "优化的步骤:1,val1:0.8999999761581421,变化值:0.10000002384185791\n",
      "优化的步骤:2,val2:0.7199999690055847,变化值:0.18000000715255737\n",
      "优化的步骤:3,val3:0.4860000014305115,变化值:0.23399996757507324\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-09T14:28:54.375418Z",
     "start_time": "2025-04-09T14:28:53.754811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "    效果衡量指标\n",
    "'''\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.metrics import precision_score,recall_score,confusion_matrix\n",
    "\n",
    "y_true = [0,0,0,1,1,1,1,1] #实际值\n",
    "y_pred = [0,1,0,1,0,1,0,1] #预测值\n",
    "\n",
    "# 混淆矩阵\n",
    "tn,fp,fn,tp = confusion_matrix(y_true,y_pred).ravel()\n",
    "print(f'tn:{tn},fp:{fp},fn:{fn},tp:{tp}')\n",
    "\n",
    "#绘图\n",
    "#修正中文问题\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "#显示矩阵\n",
    "fig, ax = plt.subplots(figsize=(2.5,2.5))\n",
    "\n",
    "# 1:蓝色;0:白色\n",
    "ax.matshow([[1,0],[0,1]], cmap=plt.cm.Blues,alpha=0.3)\n",
    "\n",
    "#标示文字\n",
    "ax.text(x = 0, y =0,s = 'tp',va = 'center',ha = 'center')\n",
    "ax.text(x = 1, y =0,s = 'fp',va = 'center',ha = 'center')\n",
    "ax.text(x = 0, y =1,s = 'tn',va = 'center',ha = 'center')\n",
    "ax.text(x = 1, y =1,s = 'fn',va = 'center',ha = 'center')\n",
    "\n",
    "plt.xlabel('实际',fontsize=20)\n",
    "plt.ylabel('预测',fontsize=20)\n",
    "\n",
    "# x/y标签\n",
    "plt.xticks([0,1],['T','F'],fontsize=20)\n",
    "plt.yticks([0,1],['P','N'],fontsize=20)\n",
    "\n",
    "plt.show()"
   ],
   "id": "a0bacb50cd65ac8c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tn:2,fp:1,fn:2,tp:3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 250x250 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "7b34dadc069de70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
